{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90233a93-8670-4ebd-9fc2-39807c29b341",
   "metadata": {},
   "source": [
    "# Setup, Training, and Evaluating CNNs for Tree Genera Classification\n",
    "\n",
    "The following Jupyter Notebook includes code to setup and train a convolutional neural network with Python and Pytorch.\n",
    "\n",
    "Version: April 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34f37d09-22ad-4965-89ea-63af7fae1da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Imports for Pytorch\n",
    "import torch # version 2.1.2\n",
    "import torchvision # version 0.16.2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn # contains base class for all neural network modules\n",
    "import torch.nn.functional as F #https://pytorch.org/docs/stable/nn.functional.html contains common functions for training NNs (convolutions, losses, etc..)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Image processing and display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from IPython.display import clear_output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "# Other Imports\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import time\n",
    "from imageio import imread\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698887d",
   "metadata": {},
   "source": [
    "## Setup new testing and training datasets, save their metadata as csvs, and print number of images to console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a43c91-ae82-44a0-a622-baa3d2490ed9",
   "metadata": {},
   "source": [
    "### Functions to create new testing and training datasets, export metadata, and print number of images to the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb6fbfe-b7d0-46c3-9481-9f217c19a612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Functions to create datasets for training and testing, export metadata to csv, and print directory information\n",
    "# Function to export metadata to csv\n",
    "def process_existing_files(root_directory, csv_path, export_csv=False):\n",
    "    \"\"\"\n",
    "    Process existing files in the root directory and write to csv.\n",
    "\n",
    "    Parameters:\n",
    "    root_directory (str): The path to the root directory containing images of tree genera.\n",
    "\n",
    "    Returns:\n",
    "    Print statement with the file path to the csv.\n",
    "    \"\"\"\n",
    "    existing_files = {}\n",
    "\n",
    "    # Get the list of genus names from the root directory\n",
    "    genera = os.listdir(root_directory)\n",
    "\n",
    "    for genus in genera:\n",
    "        genus_dir = os.path.join(root_directory, genus)\n",
    "        genus_files = os.listdir(genus_dir)\n",
    "        existing_files[genus] = genus_files\n",
    "\n",
    "    # Create a dataframe from the dictionary\n",
    "    df = pd.DataFrame(existing_files.items(), columns=['genus', 'files'])\n",
    "    df = df.explode('files')\n",
    "\n",
    "    # Add additional column for data source\n",
    "    if \"inat\" in root_directory.lower():\n",
    "        df['data_source'] = \"iNaturalist\"\n",
    "    else:\n",
    "        df['data_source'] = \"Autoarborist\"\n",
    "    \n",
    "    if export_csv:\n",
    "        # Write to csv\n",
    "        csv_filepath = os.path.join(csv_path, os.path.basename(root_directory) + \".csv\")\n",
    "        df.to_csv(csv_filepath, index=False)\n",
    "        print(f\"Existing files for {root_directory} written to csv.\")\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "# Function to print directory information\n",
    "def print_directory_info(root_directory):\n",
    "    \"\"\"\n",
    "    Print the number of files in each directory in the root directory.\n",
    "\n",
    "    Parameters:\n",
    "    root_directory (str): The path to the root directory containing images of tree genera.\n",
    "\n",
    "    Returns:\n",
    "    Print statement with the number of files in each directory.\n",
    "    \"\"\"\n",
    "    for genus_folder in os.listdir(root_directory):\n",
    "        genus_path = os.path.join(root_directory, genus_folder)\n",
    "        \n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(genus_path):\n",
    "            # Count the number of files in the directory\n",
    "            num_files = len([f for f in os.listdir(genus_path) if os.path.isfile(os.path.join(genus_path, f))])\n",
    "            \n",
    "            print(f\"Directory: {genus_folder}, Number of Files: {num_files}\")\n",
    "    return None\n",
    "\n",
    "# Function to create datasets for training and testing: Autoarborist or iNaturalist\n",
    "def create_datasets (training_ratio, max_training_images_og, max_testing_images_og, selected_genera, source_root, testing_destination_root, \n",
    "                     training_destination_root, existing_training_root, existing_testing_root, append, csv_path):\n",
    "    \"\"\"\n",
    "    Create training and testing datasets for selected genera from any source image dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    training_ratio (float): The ratio of training images to total images.\n",
    "    max_training_images_og (int): The maximum number of training images to select.\n",
    "    max_testing_images_og (int): The maximum number of testing images to select.\n",
    "    selected_genera (list): The list of selected genera to include in the training and testing datasets.\n",
    "    source_root (str): The path to the source directory containing all available images of tree genera from Autoarborist.\n",
    "    testing_destination_root (str): The path to the destination directory for images of tree genera as testing data.\n",
    "    training_destination_root (str): The path to the destination directory for images of tree genera as training data.\n",
    "    existing_training_root (str): The path to the existing directory containing images of tree genera as training data used in previous experiments.\n",
    "    existing_testing_root (str): The path to the existing directory containing images of tree genera as testing data used in previous experiments.\n",
    "    append (bool): A logical statement to append new images to existing training and testing data.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Iterate through the source directory\n",
    "    for genus_folder in os.listdir(source_root):\n",
    "        max_training_images = max_training_images_og\n",
    "        max_testing_images = max_testing_images_og\n",
    "        # Keep track of starting time\n",
    "        start_time = time.time()\n",
    "        genus_path = os.path.join(source_root, genus_folder) # Get path to images for each genera\n",
    "    \n",
    "        # List all images in the current genus folder. Some images are .jpg and .jpeg format.\n",
    "        # If \"inat\" is found anywhere in the genus folder name, then the images are in the root folder.\n",
    "        if \"inat\" in genus_path.lower():\n",
    "            images = [image for image in os.listdir(genus_path) if image.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        else:\n",
    "            images = [image for image in os.listdir(os.path.join(genus_path, 'images')) if image.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        # Only select genera with >100 images.\n",
    "        if len(images) > 100:\n",
    "            # Check if it's a directory and if it's in the selected genera list\n",
    "            if os.path.isdir(genus_path) and genus_folder in selected_genera:\n",
    "                # Create destination folders for the training and testing data for the current genus\n",
    "                training_destination_genus_path = os.path.join(training_destination_root, genus_folder)\n",
    "                testing_destination_genus_path = os.path.join(testing_destination_root, genus_folder)\n",
    "                os.makedirs(training_destination_genus_path, exist_ok=True)\n",
    "                os.makedirs(testing_destination_genus_path, exist_ok=True)\n",
    "                # Append new images to existing training and testing data\n",
    "                if append:\n",
    "                    print(f\"Copying existing images for {genus_folder} to new training and testing folders.\")\n",
    "                    existing_training_genus_path = os.path.join(existing_training_root, genus_folder)\n",
    "                    existing_testing_genus_path = os.path.join(existing_testing_root, genus_folder)\n",
    "\n",
    "                    # Copy existing training images to the new training destination folder\n",
    "                    for image in os.listdir(existing_training_genus_path):\n",
    "                        source_image_path = os.path.join(existing_training_genus_path, image)\n",
    "                        destination_image_path = os.path.join(training_destination_genus_path, image)\n",
    "                        _= shutil.copy2(source_image_path, destination_image_path)\n",
    "\n",
    "                    # Copy existing testing images to the new testing destination folder\n",
    "                    for image in os.listdir(existing_testing_genus_path):\n",
    "                        source_image_path = os.path.join(existing_testing_genus_path, image)\n",
    "                        destination_image_path = os.path.join(testing_destination_genus_path, image)\n",
    "                        _= shutil.copy2(source_image_path, destination_image_path)\n",
    "\n",
    "                    # Update the max_training_images and max_testing_images\n",
    "                    max_training_images = max_training_images - len(os.listdir(existing_training_genus_path))\n",
    "                    max_testing_images = max_testing_images - len(os.listdir(existing_testing_genus_path))\n",
    "                    print(f\"Updated max_training_images: {max_training_images}, max_testing_images: {max_testing_images}\")\n",
    "\n",
    "                    # Update images to exclude the existing training and testing images\n",
    "                    print(f\"Total number of available images: {len(images)} for {genus_folder}...\")\n",
    "                    existing_training_images = set(os.listdir(existing_training_genus_path))\n",
    "                    existing_testing_images = set(os.listdir(existing_testing_genus_path))\n",
    "                    images = [image for image in images if image not in existing_training_images and image not in existing_testing_images]\n",
    "                    print(f\"Total number of images after excluding existing training and testing images: {len(images)} for {genus_folder}...\")\n",
    "\n",
    "                # Randomly select a number of images from the folder here: (900 training + 100 testing).\n",
    "                if len(images) > max_training_images + max_testing_images:\n",
    "                    images = random.sample(images, max_training_images + max_testing_images) # file paths for images\n",
    "\n",
    "                # Randomly divide images into training and testing sets\n",
    "                num_total_images = len(images)\n",
    "                num_training_images_to_copy = min(int(num_total_images * training_ratio), max_training_images)\n",
    "\n",
    "                # Randomly shuffle the images before moving\n",
    "                random.shuffle(images)\n",
    "\n",
    "                # Split images into training and testing sets\n",
    "                training_images = images[:num_training_images_to_copy]\n",
    "                testing_images = images[num_training_images_to_copy:]\n",
    "            \n",
    "                # Copy training images to the training destination folder\n",
    "                print(f\"Copying new images for {genus_folder} to new training and testing folders.\")\n",
    "                for image in training_images:\n",
    "                    if \"inat\" in genus_path.lower():\n",
    "                        source_image_path = os.path.join(genus_path, image)\n",
    "                    else:\n",
    "                        source_image_path = os.path.join(genus_path, 'images', image)\n",
    "                    destination_image_path = os.path.join(training_destination_genus_path, image)\n",
    "                    _= shutil.copy2(source_image_path, destination_image_path)\n",
    "\n",
    "                # Copy testing images to the testing destination folder\n",
    "                for image in testing_images:\n",
    "                    if \"inat\" in genus_path.lower():\n",
    "                        source_image_path = os.path.join(genus_path, image)\n",
    "                    else:\n",
    "                        source_image_path = os.path.join(genus_path, 'images', image)\n",
    "                    destination_image_path = os.path.join(testing_destination_genus_path, image)\n",
    "                    _= shutil.copy2(source_image_path, destination_image_path)\n",
    "                # Keep track of ending time\n",
    "                end_time = time.time()\n",
    "                # Report time take in minutes\n",
    "                total_time = (end_time - start_time) / 60\n",
    "                print(f\"Images copied successfully for {genus_folder}. Time taken: {total_time} minutes.\")\n",
    "    print(f\"All images copied successfully for: {selected_genera}.\")\n",
    "    process_existing_files(training_destination_root, csv_path, export_csv=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Function to combine existing training and testing data\n",
    "\n",
    "def combine_datasets(autoarborist_dataset_root, inaturalist_dataset_root, combined_dataset_root):\n",
    "    \"\"\"\n",
    "    Combine the training datasets from Autoarborist and iNaturalist into a single testing or training dataset.\n",
    "    Combine the testing datasets from Autoarborist and iNaturalist into a single testing or training dataset.\n",
    "    Combine the metadata from Autoarborist and iNaturalist into a single metadata file.\n",
    "\n",
    "    Parameters:\n",
    "    autoarborist_dataset_root (str): The path to the Autoarborist testing or training dataset.\n",
    "    inaturalist_dataset_root (str): The path to the iNaturalist testing or training dataset.\n",
    "    combined_dataset_root (str): The path to the combined testing or training dataset.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create the combined training dataset directory\n",
    "    os.makedirs(combined_dataset_root, exist_ok=True)\n",
    "\n",
    "    # Copy the Autoarborist dataset to the combined dataset\n",
    "    for genus_folder in os.listdir(autoarborist_dataset_root):\n",
    "        genus_path = os.path.join(autoarborist_dataset_root, genus_folder)\n",
    "        destination_genus_path = os.path.join(combined_dataset_root, genus_folder)\n",
    "        os.makedirs(destination_genus_path, exist_ok=True)\n",
    "        print(f\"Copying Autoarborist images for {genus_folder} to the combined dataset.\")\n",
    "\n",
    "        for image in os.listdir(genus_path):\n",
    "            source_image_path = os.path.join(genus_path, image)\n",
    "            destination_image_path = os.path.join(destination_genus_path, image)\n",
    "            _= shutil.copy2(source_image_path, destination_image_path)\n",
    "        \n",
    "    \n",
    "    # Copy the iNaturalist dataset to the training dataset\n",
    "    for genus_folder in os.listdir(inaturalist_dataset_root):\n",
    "        genus_path = os.path.join(inaturalist_dataset_root, genus_folder)\n",
    "        destination_genus_path = os.path.join(combined_dataset_root, genus_folder)\n",
    "        os.makedirs(destination_genus_path, exist_ok=True)\n",
    "        print(f\"Copying iNaturalist images for {genus_folder} to the combined dataset.\")\n",
    "\n",
    "        for image in os.listdir(genus_path):\n",
    "            source_image_path = os.path.join(genus_path, image)\n",
    "            destination_image_path = os.path.join(destination_genus_path, image)\n",
    "            _= shutil.copy2(source_image_path, destination_image_path)\n",
    "\n",
    "    print(f\"Combined dataset created successfully.\")\n",
    "\n",
    "    # Combine the autoarborist_root, inaturalist_root, and combined_root metadata\n",
    "    roots = [autoarborist_dataset_root, inaturalist_dataset_root, combined_dataset_root]\n",
    "    updated_roots = [os.path.join(os.path.join(root.rsplit('\\\\', 1)[0], os.path.basename(root) + \".csv\")) for root in roots]\n",
    "    \n",
    "    # Pull in the existing metadata for autoarborist\n",
    "    autoarborist_metadata = pd.read_csv(updated_roots[0])\n",
    "    # Pull in the existing metadata for inaturalist\n",
    "    inaturalist_metadata = pd.read_csv(updated_roots[1])\n",
    "    # Combine the metadata\n",
    "    combined_metadata = pd.concat([autoarborist_metadata, inaturalist_metadata], ignore_index=True)\n",
    "    # Write to csv\n",
    "    combined_metadata.to_csv(updated_roots[2], index=False)\n",
    "    print(f\"Combined metadata written to csv.\")\n",
    "    return None\n",
    "\n",
    "# Constants\n",
    "selected_genera = ['acer','ailanthus','betula','citrus','fraxinus','gleditsia','juglans','juniperus', 'magnolia','phoenix','picea',\n",
    "                   'pinus','prunus','pseudotsuga','pyrus','quercus','rhus','sequoia','taxodium', 'thuja','tilia','ulmus','washingtonia']\n",
    "training_ratio = 0.9 # ratio of training images to total images\n",
    "max_training_images_og = 1187 # maximum number of training images to select\n",
    "max_testing_images_og = 132 # maximum number of testing images to select\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe88f9",
   "metadata": {},
   "source": [
    "### Autoarborist: Source, append, and create new testing and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: contains all available street view images of tree genera from Autoarborist\n",
    "source_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\autoarborist_original_data\\autoarborist_original_jpegs\\jpegs_streetlevel_genus_idx_label\"\n",
    "\n",
    "# Target: location for images of tree genera as training data \n",
    "training_destination_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\autoarborist\\training_dataset_small_apr624\"\n",
    "\n",
    "# Target: location for images of tree genera as testing data \n",
    "testing_destination_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\autoarborist\\testing_dataset_small_apr624\"\n",
    "\n",
    "# Existing Target: location for existing images of tree genera as training data used in previous experiments\n",
    "existing_training_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\autoarborist\\training_dataset_small_march624\"\n",
    "\n",
    "# Existing Target: location for existing images of tree genera as testing data used in previous experiments\n",
    "existing_testing_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\autoarborist\\testing_dataset_small_march624\"\n",
    "\n",
    "# Append: logical statement to append new images to existing training and testing data\n",
    "append = True\n",
    "\n",
    "# CSV path: location to save the CSV file containing the training and testing data\n",
    "csv_path = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\autoarborist\"\n",
    "\n",
    "# Select all genera\n",
    "#selected_genera = os.listdir(source_root)\n",
    "# create_datasets(training_ratio, max_training_images_og, max_testing_images_og, selected_genera, source_root, testing_destination_root,\n",
    "#                     training_destination_root, existing_training_root, existing_testing_root, append)\n",
    "\n",
    "# Export metadata for existing training and testing data\n",
    "process_existing_files(training_destination_root, csv_path, export_csv=True)\n",
    "process_existing_files(testing_destination_root, csv_path, export_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1099f",
   "metadata": {},
   "source": [
    "### Autoarborist: Print the number of images for the new training and testing directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c99d6-6169-4053-933c-601f72b02f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print information for the training directory\n",
    "print(\"Training Directory Information:\")\n",
    "print_directory_info(training_destination_root)\n",
    "\n",
    "# Print information for the testing directory\n",
    "print(\"/nTesting Directory Information:\")\n",
    "print_directory_info(testing_destination_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc72c8",
   "metadata": {},
   "source": [
    "### iNaturalist: Source, append, and create new testing and training datasets for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3afcf1-3e9f-4c50-b543-ba359c2e04a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the paths for Autoarborist training and testing data\n",
    "\n",
    "# Source: contains all available street view images of tree genera from iNaturalist\n",
    "source_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\inat\\images\\original_10k\"\n",
    "\n",
    "# Target: location for images of tree genera as training data \n",
    "training_destination_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\inaturalist\\training_dataset_small_apr624\"\n",
    "\n",
    "# Target: location for images of tree genera as testing data \n",
    "testing_destination_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\inaturalist\\testing_dataset_small_apr624\"\n",
    "\n",
    "# Existing Target: location for existing images of tree genera as training data used in previous experiments\n",
    "existing_training_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\inaturalist\\training_dataset_small_apr624\"\n",
    "\n",
    "# Existing Target: location for existing images of tree genera as testing data used in previous experiments\n",
    "existing_testing_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\inaturalist\\testing_dataset_small_apr624\"\n",
    "\n",
    "# Append: logical statement to append new images to existing training and testing data\n",
    "append = True\n",
    "\n",
    "# CSV path: location to save the CSV file containing the training and testing data\n",
    "csv_path = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\inaturalist\"\n",
    "\n",
    "# Select all genera\n",
    "selected_genera = os.listdir(source_root)\n",
    "create_datasets(training_ratio, max_training_images_og, max_testing_images_og, selected_genera, source_root, testing_destination_root,\n",
    "                    training_destination_root, existing_training_root, existing_testing_root, append,csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9a9fa",
   "metadata": {},
   "source": [
    "### iNaturalist: Print the number of images for the new training and testing directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information for the training directory\n",
    "print(\"Training Directory Information:\")\n",
    "print_directory_info(training_destination_root)\n",
    "\n",
    "# Print information for the testing directory\n",
    "print(\"/nTesting Directory Information:\")\n",
    "print_directory_info(testing_destination_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c596c9e",
   "metadata": {},
   "source": [
    "### Autoarborist + iNaturalist: Combine Training and Testing Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e80829-11c0-4fe9-8280-cbd3a30ceaed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine the training datasets\n",
    "\n",
    "# Set the paths for the combined training dataset\n",
    "autoarborist_dataset_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\autoarborist\\training_dataset_small_apr624\"\n",
    "inaturalist_dataset_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\inaturalist\\training_dataset_small_apr624\"\n",
    "combined_dataset_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\aa_inat_combined\\training_dataset_small_apr624\"\n",
    "\n",
    "# Combine the training datasets\n",
    "combine_datasets(autoarborist_dataset_root, inaturalist_dataset_root, combined_dataset_root)\n",
    "\n",
    "# Set the paths for the combined training dataset\n",
    "autoarborist_dataset_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\autoarborist\\testing_dataset_small_apr624\"\n",
    "inaturalist_dataset_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\inaturalist\\testing_dataset_small_apr624\"\n",
    "combined_dataset_root = r\"Z:\\auto_arborist_cvpr2022_v0.15\\data\\tree_classification\\aa_inat_combined\\testing_dataset_small_apr624\"\n",
    "\n",
    "# Combine the training datasets\n",
    "combine_datasets(autoarborist_dataset_root, inaturalist_dataset_root, combined_dataset_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1664a",
   "metadata": {},
   "source": [
    "### Autoarborist + iNaturalist: Print the number of images for the new training and testing directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7144d621",
   "metadata": {},
   "source": [
    "### iNaturalist (CURATED): Source, append, and create new testing and training datasets for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df5c3f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genus</th>\n",
       "      <th>Image</th>\n",
       "      <th>Softmax Probability</th>\n",
       "      <th>orig_csv_name</th>\n",
       "      <th>source_filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acer</td>\n",
       "      <td>53753889.jpg</td>\n",
       "      <td>0.946010</td>\n",
       "      <td>autoarborist_predicted_on_inat_acer_303k_image...</td>\n",
       "      <td>D:\\inat\\images\\original_full\\acer\\53753889.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acer</td>\n",
       "      <td>299592441.jpg</td>\n",
       "      <td>0.909637</td>\n",
       "      <td>autoarborist_predicted_on_inat_acer_303k_image...</td>\n",
       "      <td>D:\\inat\\images\\original_full\\acer\\299592441.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acer</td>\n",
       "      <td>87897345.jpg</td>\n",
       "      <td>0.903870</td>\n",
       "      <td>autoarborist_predicted_on_inat_acer_303k_image...</td>\n",
       "      <td>D:\\inat\\images\\original_full\\acer\\87897345.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acer</td>\n",
       "      <td>110044395.jpeg</td>\n",
       "      <td>0.899195</td>\n",
       "      <td>autoarborist_predicted_on_inat_acer_303k_image...</td>\n",
       "      <td>D:\\inat\\images\\original_full\\acer\\110044395.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acer</td>\n",
       "      <td>97287883.jpg</td>\n",
       "      <td>0.899188</td>\n",
       "      <td>autoarborist_predicted_on_inat_acer_303k_image...</td>\n",
       "      <td>D:\\inat\\images\\original_full\\acer\\97287883.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Genus           Image  Softmax Probability  \\\n",
       "0  acer    53753889.jpg             0.946010   \n",
       "1  acer   299592441.jpg             0.909637   \n",
       "2  acer    87897345.jpg             0.903870   \n",
       "3  acer  110044395.jpeg             0.899195   \n",
       "4  acer    97287883.jpg             0.899188   \n",
       "\n",
       "                                       orig_csv_name  \\\n",
       "0  autoarborist_predicted_on_inat_acer_303k_image...   \n",
       "1  autoarborist_predicted_on_inat_acer_303k_image...   \n",
       "2  autoarborist_predicted_on_inat_acer_303k_image...   \n",
       "3  autoarborist_predicted_on_inat_acer_303k_image...   \n",
       "4  autoarborist_predicted_on_inat_acer_303k_image...   \n",
       "\n",
       "                                    source_filepath  \n",
       "0    D:\\inat\\images\\original_full\\acer\\53753889.jpg  \n",
       "1   D:\\inat\\images\\original_full\\acer\\299592441.jpg  \n",
       "2    D:\\inat\\images\\original_full\\acer\\87897345.jpg  \n",
       "3  D:\\inat\\images\\original_full\\acer\\110044395.jpeg  \n",
       "4    D:\\inat\\images\\original_full\\acer\\97287883.jpg  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talake2\\AppData\\Local\\Temp\\ipykernel_34420\\1771539121.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_1000_images = combined_softmax.groupby('Genus').apply(lambda x: x.nlargest(1000, 'Softmax Probability')).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images copied successfully for acer.\n",
      "Images copied successfully for ailanthus.\n",
      "Images copied successfully for betula.\n",
      "Images copied successfully for citrus.\n",
      "Images copied successfully for cupaniopsis.\n",
      "Images copied successfully for erythrina.\n",
      "Images copied successfully for fraxinus.\n",
      "Images copied successfully for gleditsia.\n",
      "Images copied successfully for juglans_nigra.\n",
      "Images copied successfully for juniperus.\n",
      "Images copied successfully for magnolia.\n",
      "Images copied successfully for phoenix.\n",
      "Images copied successfully for picea.\n",
      "Images copied successfully for pinus.\n",
      "Images copied successfully for prunus.\n",
      "Images copied successfully for pseudotsuga.\n",
      "Images copied successfully for pyrus.\n",
      "Images copied successfully for quercus.\n",
      "Images copied successfully for rhus.\n",
      "Images copied successfully for sequoia.\n",
      "Images copied successfully for taxodium.\n",
      "Images copied successfully for thuja.\n",
      "Images copied successfully for tilia.\n",
      "Images copied successfully for ulmus.\n",
      "Images copied successfully for washingtonia.\n",
      "Existing files for D:\\blaginh\\tree_classification\\inaturalist\\training_data_top1000_may724_curated written to csv.\n",
      "Existing files for D:\\blaginh\\tree_classification\\inaturalist\\testing_data_top1000_may724_curated written to csv.\n"
     ]
    }
   ],
   "source": [
    "# # Set the paths for iNaturalist training and testing data\n",
    "# Setup file paths for the softmax csvs\n",
    "acer_path = r\"Z:\\auto_arborist_cvpr2022_v0.15\\analyses\\tree_classification\\pytorch_cnn_classifier_autoarborist_inaturalist_experiments_march2024\\autoarborist_predicted_on_inat_acer_303k_images_probability_scores_apr2924.csv\"\n",
    "aila_path = r\"Z:\\auto_arborist_cvpr2022_v0.15\\analyses\\tree_classification\\pytorch_cnn_classifier_autoarborist_inaturalist_experiments_march2024\\autoarborist_predicted_on_inat_ailanthus_48k_image_probability_scores_apr2924.csv\"\n",
    "quer_path = r\"Z:\\auto_arborist_cvpr2022_v0.15\\analyses\\tree_classification\\pytorch_cnn_classifier_autoarborist_inaturalist_experiments_march2024\\autoarborist_predicted_on_inat_quercus_286k_image_probability_scores_apr2924.csv\"\n",
    "inat_10k_path = r\"Z:\\auto_arborist_cvpr2022_v0.15\\analyses\\tree_classification\\pytorch_cnn_classifier_autoarborist_inaturalist_experiments_march2024\\autoarborist_predicted_on_inat_10kimgs_genus_image_probability_scores_march2024.csv\"\n",
    "\n",
    "\n",
    "# Pull in softmax csvs file for iNaturalist\n",
    "acer_full = pd.read_csv(acer_path)\n",
    "aila_full = pd.read_csv(aila_path)\n",
    "quer_full = pd.read_csv(quer_path)\n",
    "inat_10k = pd.read_csv(inat_10k_path)\n",
    "\n",
    "\n",
    "# Drop \"acer\", \"ailanthus\", \"quercus\" from the inat_10k dataframe\n",
    "inat_10k = inat_10k[~inat_10k['Genus'].isin([\"acer\", \"ailanthus\", \"quercus\"])]\n",
    "\n",
    "# Add base directory to the softmax csvs to keep track when combining\n",
    "\n",
    "# Add a new column 'orig_csv_name' contains the original csv file name\n",
    "acer_full['orig_csv_name'] = os.path.basename(acer_path)\n",
    "aila_full['orig_csv_name'] = os.path.basename(aila_path)\n",
    "quer_full['orig_csv_name'] = os.path.basename(quer_path)\n",
    "inat_10k['orig_csv_name'] = os.path.basename(inat_10k_path)\n",
    "\n",
    "# Add a new column 'source_root' that contains the source root directory\n",
    "source_root_full = r\"D:\\inat\\images\\original_full\"\n",
    "source_root_10k = r\"D:\\inat\\images\\original_10k\"\n",
    "\n",
    "# Add a new column 'source_filepath' that contains the joined 'source_root_full', 'Genus', and 'Image' column values\n",
    "acer_full['source_filepath'] = acer_full.apply(lambda row: os.path.join(source_root_full, row['Genus'], row['Image']), axis=1)\n",
    "aila_full['source_filepath'] = aila_full.apply(lambda row: os.path.join(source_root_full, row['Genus'], row['Image']), axis=1)\n",
    "quer_full['source_filepath'] = quer_full.apply(lambda row: os.path.join(source_root_full, row['Genus'], row['Image']), axis=1)\n",
    "inat_10k['source_filepath'] = inat_10k.apply(lambda row: os.path.join(source_root_10k, row['Genus'], row['Image']), axis=1)\n",
    "\n",
    "# Print head of the softmax csvs\n",
    "acer_full.head()\n",
    " \n",
    "# Merge the softmax csvs\n",
    "combined_softmax = pd.concat([acer_full, aila_full, quer_full, inat_10k], ignore_index=True)\n",
    "\n",
    "# Write the combined softmax csv to a file\n",
    "combined_softmax.to_csv(r\"Z:\\auto_arborist_cvpr2022_v0.15\\analyses\\tree_classification\\pytorch_cnn_classifier_autoarborist_inaturalist_experiments_march2024\\combined_softmax_5724.csv\", index=False)\n",
    "\n",
    "# Pull out the top 1000 images using the Softmax Probability column and group by Genus\n",
    "top_1000_images = combined_softmax.groupby('Genus').apply(lambda x: x.nlargest(1000, 'Softmax Probability')).reset_index(drop=True)\n",
    "\n",
    "# Write the top 1000 images to a file\n",
    "top_1000_images.to_csv(r\"Z:\\auto_arborist_cvpr2022_v0.15\\analyses\\tree_classification\\pytorch_cnn_classifier_autoarborist_inaturalist_experiments_march2024\\top_1000_images_5724.csv\", index=False)\n",
    "\n",
    "# Compute 50th, 75th, 90th percentile of the Softmax Probability column by Genus for the top 1000 images and store in a new dataframe\n",
    "top_1000_images_stats = top_1000_images.groupby('Genus')['Softmax Probability'].agg(['mean', 'median', 'max', 'min']).reset_index()\n",
    "# Compute 25th, 50th, 75th, 90th percentile of the Softmax Probability column by Genus for the top 1000 images and store in a new dataframe\n",
    "percentiles = [25, 50, 75, 90]\n",
    "\n",
    "for percentile in percentiles:\n",
    "    top_1000_images_stats[f'{percentile}th percentile'] = top_1000_images.groupby('Genus')['Softmax Probability'].apply(lambda x: np.percentile(x, percentile)).reset_index(drop=True)\n",
    "\n",
    "# Write the top 1000 images stats to a file\n",
    "top_1000_images_stats.to_csv(r\"Z:\\auto_arborist_cvpr2022_v0.15\\analyses\\tree_classification\\pytorch_cnn_classifier_autoarborist_inaturalist_experiments_march2024\\top_1000_images_stats_5724.csv\", index=False)\n",
    "\n",
    "# Filter the top_1000_images by Genus, then randomly select 900 images for training and 100 images for testing, then copy the images to the training and testing directories\n",
    "\n",
    "training_destination_root = r\"D:\\blaginh\\tree_classification\\inaturalist\\training_data_top1000_may724_curated\"\n",
    "testing_destination_root = r\"D:\\blaginh\\tree_classification\\inaturalist\\testing_data_top1000_may724_curated\"\n",
    "\n",
    "# Create the training and testing directories\n",
    "os.makedirs(training_destination_root, exist_ok=True)\n",
    "os.makedirs(testing_destination_root, exist_ok=True)\n",
    "\n",
    "# Iterate through the top_1000_images dataframe\n",
    "\n",
    "for genus in top_1000_images['Genus'].unique():\n",
    "    genus_images = top_1000_images[top_1000_images['Genus'] == genus]\n",
    "    genus_training_images = genus_images.sample(n=900)\n",
    "    genus_testing_images = genus_images.drop(genus_training_images.index)\n",
    "    \n",
    "    # Create the genus training and testing directories\n",
    "    genus_training_destination = os.path.join(training_destination_root, genus)\n",
    "    genus_testing_destination = os.path.join(testing_destination_root, genus)\n",
    "    \n",
    "    os.makedirs(genus_training_destination, exist_ok=True)\n",
    "    os.makedirs(genus_testing_destination, exist_ok=True)\n",
    "    \n",
    "    # Copy the training images\n",
    "    for index, row in genus_training_images.iterrows():\n",
    "        source_image_path = os.path.normpath(row['source_filepath'])\n",
    "        destination_image_path = os.path.join(genus_training_destination, row['Image'])\n",
    "        _= shutil.copy2(source_image_path, destination_image_path)\n",
    "    \n",
    "    # Copy the testing images\n",
    "    for index, row in genus_testing_images.iterrows():\n",
    "        source_image_path = os.path.normpath(row['source_filepath'])\n",
    "        destination_image_path = os.path.join(genus_testing_destination, row['Image'])\n",
    "        _= shutil.copy2(source_image_path, destination_image_path)\n",
    "    print(f\"Images copied successfully for {genus}.\")\n",
    "training_destination_root = r\"D:\\blaginh\\tree_classification\\inaturalist\\training_data_top1000_may724_curated\"\n",
    "testing_destination_root = r\"D:\\blaginh\\tree_classification\\inaturalist\\testing_data_top1000_may724_curated\"\n",
    "csv_path = r\"D:\\blaginh\\tree_classification\\inaturalist\"\n",
    "process_existing_files(training_destination_root, csv_path, export_csv=True)\n",
    "process_existing_files(testing_destination_root, csv_path, export_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8e81f",
   "metadata": {},
   "source": [
    "### iNaturalist (CURATED): Print the number of images for the new training and testing directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2d30909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Directory Information:\n",
      "Directory: acer, Number of Files: 900\n",
      "Directory: ailanthus, Number of Files: 900\n",
      "Directory: betula, Number of Files: 900\n",
      "Directory: citrus, Number of Files: 900\n",
      "Directory: cupaniopsis, Number of Files: 900\n",
      "Directory: erythrina, Number of Files: 900\n",
      "Directory: fraxinus, Number of Files: 900\n",
      "Directory: gleditsia, Number of Files: 900\n",
      "Directory: juglans, Number of Files: 900\n",
      "Directory: juniperus, Number of Files: 900\n",
      "Directory: magnolia, Number of Files: 900\n",
      "Directory: phoenix, Number of Files: 900\n",
      "Directory: picea, Number of Files: 900\n",
      "Directory: pinus, Number of Files: 900\n",
      "Directory: prunus, Number of Files: 900\n",
      "Directory: pseudotsuga, Number of Files: 900\n",
      "Directory: pyrus, Number of Files: 900\n",
      "Directory: quercus, Number of Files: 900\n",
      "Directory: rhus, Number of Files: 900\n",
      "Directory: sequoia, Number of Files: 900\n",
      "Directory: taxodium, Number of Files: 900\n",
      "Directory: thuja, Number of Files: 900\n",
      "Directory: tilia, Number of Files: 900\n",
      "Directory: ulmus, Number of Files: 900\n",
      "Directory: washingtonia, Number of Files: 900\n",
      "/nTesting Directory Information:\n",
      "Directory: acer, Number of Files: 100\n",
      "Directory: ailanthus, Number of Files: 100\n",
      "Directory: betula, Number of Files: 100\n",
      "Directory: citrus, Number of Files: 100\n",
      "Directory: cupaniopsis, Number of Files: 100\n",
      "Directory: erythrina, Number of Files: 100\n",
      "Directory: fraxinus, Number of Files: 100\n",
      "Directory: gleditsia, Number of Files: 100\n",
      "Directory: juglans, Number of Files: 100\n",
      "Directory: juniperus, Number of Files: 100\n",
      "Directory: magnolia, Number of Files: 100\n",
      "Directory: phoenix, Number of Files: 100\n",
      "Directory: picea, Number of Files: 100\n",
      "Directory: pinus, Number of Files: 100\n",
      "Directory: prunus, Number of Files: 100\n",
      "Directory: pseudotsuga, Number of Files: 100\n",
      "Directory: pyrus, Number of Files: 100\n",
      "Directory: quercus, Number of Files: 100\n",
      "Directory: rhus, Number of Files: 100\n",
      "Directory: sequoia, Number of Files: 100\n",
      "Directory: taxodium, Number of Files: 100\n",
      "Directory: thuja, Number of Files: 100\n",
      "Directory: tilia, Number of Files: 100\n",
      "Directory: ulmus, Number of Files: 100\n",
      "Directory: washingtonia, Number of Files: 100\n"
     ]
    }
   ],
   "source": [
    "training_destination_root = r\"D:\\blaginh\\tree_classification\\inaturalist\\training_data_top1000_may724_curated\"\n",
    "testing_destination_root = r\"D:\\blaginh\\tree_classification\\inaturalist\\testing_data_top1000_may724_curated\"\n",
    "\n",
    "# Specify the current directory name and the new directory name\n",
    "current_dir_name = 'juglans_nigra'\n",
    "new_dir_name = 'juglans'\n",
    "\n",
    "# Full paths for the current and new directory names\n",
    "current_dir_path = os.path.join(training_destination_root, current_dir_name)\n",
    "new_dir_path = os.path.join(training_destination_root, new_dir_name)\n",
    "\n",
    "# Rename the juglans_nigra in training folder\n",
    "os.rename(current_dir_path, new_dir_path)\n",
    "\n",
    "# Full paths for the current and new directory names\n",
    "current_dir_path = os.path.join(testing_destination_root, current_dir_name)\n",
    "new_dir_path = os.path.join(testing_destination_root, new_dir_name)\n",
    "\n",
    "# Rename the juglans_nigra in training folder\n",
    "os.rename(current_dir_path, new_dir_path)\n",
    "\n",
    "# Print information for the training directory\n",
    "print(\"Training Directory Information:\")\n",
    "print_directory_info(training_destination_root)\n",
    "\n",
    "# Print information for the testing directory\n",
    "print(\"/nTesting Directory Information:\")\n",
    "print_directory_info(testing_destination_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf570b",
   "metadata": {},
   "source": [
    "### Autoarborist + iNaturalist (CURATED): Combine Training and Testing Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4749dfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying Autoarborist images for acer to the combined dataset.\n",
      "Copying Autoarborist images for ailanthus to the combined dataset.\n",
      "Copying Autoarborist images for betula to the combined dataset.\n",
      "Copying Autoarborist images for citrus to the combined dataset.\n",
      "Copying Autoarborist images for cupaniopsis to the combined dataset.\n",
      "Copying Autoarborist images for erythrina to the combined dataset.\n",
      "Copying Autoarborist images for fraxinus to the combined dataset.\n",
      "Copying Autoarborist images for gleditsia to the combined dataset.\n",
      "Copying Autoarborist images for juglans to the combined dataset.\n",
      "Copying Autoarborist images for juniperus to the combined dataset.\n",
      "Copying Autoarborist images for magnolia to the combined dataset.\n",
      "Copying Autoarborist images for phoenix to the combined dataset.\n",
      "Copying Autoarborist images for picea to the combined dataset.\n",
      "Copying Autoarborist images for pinus to the combined dataset.\n",
      "Copying Autoarborist images for prunus to the combined dataset.\n",
      "Copying Autoarborist images for pseudotsuga to the combined dataset.\n",
      "Copying Autoarborist images for pyrus to the combined dataset.\n",
      "Copying Autoarborist images for quercus to the combined dataset.\n",
      "Copying Autoarborist images for rhus to the combined dataset.\n",
      "Copying Autoarborist images for sequoia to the combined dataset.\n",
      "Copying Autoarborist images for taxodium to the combined dataset.\n",
      "Copying Autoarborist images for thuja to the combined dataset.\n",
      "Copying Autoarborist images for tilia to the combined dataset.\n",
      "Copying Autoarborist images for ulmus to the combined dataset.\n",
      "Copying Autoarborist images for washingtonia to the combined dataset.\n",
      "Copying iNaturalist images for acer to the combined dataset.\n",
      "Copying iNaturalist images for ailanthus to the combined dataset.\n",
      "Copying iNaturalist images for betula to the combined dataset.\n",
      "Copying iNaturalist images for citrus to the combined dataset.\n",
      "Copying iNaturalist images for cupaniopsis to the combined dataset.\n",
      "Copying iNaturalist images for erythrina to the combined dataset.\n",
      "Copying iNaturalist images for fraxinus to the combined dataset.\n",
      "Copying iNaturalist images for gleditsia to the combined dataset.\n",
      "Copying iNaturalist images for juglans to the combined dataset.\n",
      "Copying iNaturalist images for juniperus to the combined dataset.\n",
      "Copying iNaturalist images for magnolia to the combined dataset.\n",
      "Copying iNaturalist images for phoenix to the combined dataset.\n",
      "Copying iNaturalist images for picea to the combined dataset.\n",
      "Copying iNaturalist images for pinus to the combined dataset.\n",
      "Copying iNaturalist images for prunus to the combined dataset.\n",
      "Copying iNaturalist images for pseudotsuga to the combined dataset.\n",
      "Copying iNaturalist images for pyrus to the combined dataset.\n",
      "Copying iNaturalist images for quercus to the combined dataset.\n",
      "Copying iNaturalist images for rhus to the combined dataset.\n",
      "Copying iNaturalist images for sequoia to the combined dataset.\n",
      "Copying iNaturalist images for taxodium to the combined dataset.\n",
      "Copying iNaturalist images for thuja to the combined dataset.\n",
      "Copying iNaturalist images for tilia to the combined dataset.\n",
      "Copying iNaturalist images for ulmus to the combined dataset.\n",
      "Copying iNaturalist images for washingtonia to the combined dataset.\n",
      "Combined dataset created successfully.\n",
      "Combined metadata written to csv.\n",
      "Copying Autoarborist images for acer to the combined dataset.\n",
      "Copying Autoarborist images for ailanthus to the combined dataset.\n",
      "Copying Autoarborist images for betula to the combined dataset.\n",
      "Copying Autoarborist images for citrus to the combined dataset.\n",
      "Copying Autoarborist images for cupaniopsis to the combined dataset.\n",
      "Copying Autoarborist images for erythrina to the combined dataset.\n",
      "Copying Autoarborist images for fraxinus to the combined dataset.\n",
      "Copying Autoarborist images for gleditsia to the combined dataset.\n",
      "Copying Autoarborist images for juglans to the combined dataset.\n",
      "Copying Autoarborist images for juniperus to the combined dataset.\n",
      "Copying Autoarborist images for magnolia to the combined dataset.\n",
      "Copying Autoarborist images for phoenix to the combined dataset.\n",
      "Copying Autoarborist images for picea to the combined dataset.\n",
      "Copying Autoarborist images for pinus to the combined dataset.\n",
      "Copying Autoarborist images for prunus to the combined dataset.\n",
      "Copying Autoarborist images for pseudotsuga to the combined dataset.\n",
      "Copying Autoarborist images for pyrus to the combined dataset.\n",
      "Copying Autoarborist images for quercus to the combined dataset.\n",
      "Copying Autoarborist images for rhus to the combined dataset.\n",
      "Copying Autoarborist images for sequoia to the combined dataset.\n",
      "Copying Autoarborist images for taxodium to the combined dataset.\n",
      "Copying Autoarborist images for thuja to the combined dataset.\n",
      "Copying Autoarborist images for tilia to the combined dataset.\n",
      "Copying Autoarborist images for ulmus to the combined dataset.\n",
      "Copying Autoarborist images for washingtonia to the combined dataset.\n",
      "Copying iNaturalist images for acer to the combined dataset.\n",
      "Copying iNaturalist images for ailanthus to the combined dataset.\n",
      "Copying iNaturalist images for betula to the combined dataset.\n",
      "Copying iNaturalist images for citrus to the combined dataset.\n",
      "Copying iNaturalist images for cupaniopsis to the combined dataset.\n",
      "Copying iNaturalist images for erythrina to the combined dataset.\n",
      "Copying iNaturalist images for fraxinus to the combined dataset.\n",
      "Copying iNaturalist images for gleditsia to the combined dataset.\n",
      "Copying iNaturalist images for juglans to the combined dataset.\n",
      "Copying iNaturalist images for juniperus to the combined dataset.\n",
      "Copying iNaturalist images for magnolia to the combined dataset.\n",
      "Copying iNaturalist images for phoenix to the combined dataset.\n",
      "Copying iNaturalist images for picea to the combined dataset.\n",
      "Copying iNaturalist images for pinus to the combined dataset.\n",
      "Copying iNaturalist images for prunus to the combined dataset.\n",
      "Copying iNaturalist images for pseudotsuga to the combined dataset.\n",
      "Copying iNaturalist images for pyrus to the combined dataset.\n",
      "Copying iNaturalist images for quercus to the combined dataset.\n",
      "Copying iNaturalist images for rhus to the combined dataset.\n",
      "Copying iNaturalist images for sequoia to the combined dataset.\n",
      "Copying iNaturalist images for taxodium to the combined dataset.\n",
      "Copying iNaturalist images for thuja to the combined dataset.\n",
      "Copying iNaturalist images for tilia to the combined dataset.\n",
      "Copying iNaturalist images for ulmus to the combined dataset.\n",
      "Copying iNaturalist images for washingtonia to the combined dataset.\n",
      "Combined dataset created successfully.\n",
      "Combined metadata written to csv.\n"
     ]
    }
   ],
   "source": [
    "# Set the paths for the combined training dataset\n",
    "autoarborist_dataset_root = r\"D:\\blaginh\\tree_classification\\autoarborist\\training_dataset_small_march624\"\n",
    "inaturalist_dataset_root = r\"D:\\blaginh\\tree_classification\\inaturalist\\training_data_top1000_may724_curated\"\n",
    "combined_dataset_root = r\"D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\"\n",
    "\n",
    "# Combine the training datasets\n",
    "combine_datasets(autoarborist_dataset_root, inaturalist_dataset_root, combined_dataset_root)\n",
    "\n",
    "# Set the paths for the combined training dataset\n",
    "autoarborist_dataset_root = r\"D:\\blaginh\\tree_classification\\autoarborist\\testing_dataset_small_march624\"\n",
    "inaturalist_dataset_root = r\"D:\\blaginh\\tree_classification\\inaturalist\\testing_data_top1000_may724_curated\"\n",
    "combined_dataset_root = r\"D:\\blaginh\\tree_classification\\aa_inat_combined\\testing_data_top1000_may724_curated\"\n",
    "\n",
    "# Combine the training datasets\n",
    "combine_datasets(autoarborist_dataset_root, inaturalist_dataset_root, combined_dataset_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62102bfd",
   "metadata": {},
   "source": [
    "## Setup EfficientNetv2 model to run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c52ebf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to prepare a Basic Model for Image Classification\n",
    "\n",
    "class ImageClassificationBase(nn.Module): # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
    "    # Define a base class with functionality for model training, validation, and evaluation per epoch\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images) # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images) # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        acc = accuracy(out, labels) # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "# Functions to define a CNN Model using EfficientNetV2-S\n",
    "\n",
    "class EfficientNetImageClassification(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained EfficientNetV2-L Model\n",
    "        self.network = torchvision.models.efficientnet_v2_s(pretrained=True)\n",
    "        # Modify the final fully connected layer to match the number of classes in your dataset\n",
    "        num_classes = len(train_dataset.classes)\n",
    "        in_features = self.network.classifier[1].in_features\n",
    "        self.network.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n",
    "# Functions  to visualize training data\n",
    "def display_img(img,label):\n",
    "    print(f\"Label : {train_dataset.classes[label]}\")\n",
    "    plt.imshow(img.permute(1,2,0)) #reshape image from (3, H, W) to (H, W, 3)\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for images, labels in dl:\n",
    "        fig,ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "        break\n",
    "\n",
    "# Functions to define GPU device and load data to GPU\n",
    "def get_default_device():\n",
    "    \"\"\" Set Device to GPU or CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "\n",
    "def to_device(data, device):\n",
    "    \"Move data to the device\"\n",
    "    if isinstance(data,(list,tuple)):\n",
    "        return [to_device(x,device) for x in data]\n",
    "    return data.to(device,non_blocking = True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\" Wrap a dataloader to move data to a device \"\"\"\n",
    "    \n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\" Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b,self.device)\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\" Number of batches \"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "# Functions to define training and evaluation functions\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "# Fit model\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    # Create optimizer with initial learning rate\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        print(f\"Starting epoch {epoch+1}\")\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Forward pass: prediction & calculate loss\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            # Backward pass: backpropagate loss & calculate gradients\n",
    "            loss.backward()\n",
    "            optimizer.step() #update gradients\n",
    "            optimizer.zero_grad() #zero gradients for next training forward pass\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        end = time.time()\n",
    "        total = (end-start)/60\n",
    "        print(f\"Time taken for epoch {epoch+1}: {total} minutes\")\n",
    "        \n",
    "    return history\n",
    "\n",
    "# Functions to visualize the results\n",
    "\n",
    "def plot_accuracies(history):\n",
    "    \"\"\" Plot the history of accuracies\"\"\"\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs')\n",
    "    # Reduce plot margins\n",
    "    plt.autoscale()\n",
    "    plt.margins(0.2)\n",
    "    plt.show()\n",
    "\n",
    "def plot_losses(history):\n",
    "    \"\"\" Plot the losses in each epoch\"\"\"\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs')\n",
    "    # Reduce plot margins\n",
    "    plt.autoscale()\n",
    "    plt.margins(0.2)\n",
    "    plt.show()\n",
    "\n",
    "# Constant variables: number of epochs, optimizer function, learning rate, warmup epochs, and training and testing data paths\n",
    "selected_genera = ['acer','ailanthus','betula','citrus','cupaniopsis','erythrina','fraxinus','gleditsia','juglans','juniperus',\n",
    "                   'magnolia','phoenix','picea','pinus','prunus','pseudotsuga','pyrus','quercus','rhus','sequoia','taxodium',\n",
    "                   'thuja','tilia','ulmus','washingtonia']\n",
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "base_lr = 0.001\n",
    "training_destination_root = r\"D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\"\n",
    "testing_destination_root = r\"D:\\blaginh\\tree_classification\\aa_inat_combined\\testing_data_top1000_may724_curated\"\n",
    "testing_destination_root_aa = r\"D:\\blaginh\\tree_classification\\autoarborist\\testing_dataset_small_march624\"\n",
    "testing_destination_root_inat = r\"D:\\blaginh\\tree_classification\\inaturalist\\testing_data_top1000_may724_curated\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc2b89",
   "metadata": {},
   "source": [
    "### Compute summary statistics for Normalize transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d4da9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0005ab29-b9b2-3f61-a764-7a04a6522771.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0005ab29-b9b2-3f61-a764-7a04a6522771.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\01b21349-747f-3d7d-af77-4e54731dc16f.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\01b21349-747f-3d7d-af77-4e54731dc16f.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\01fbb20e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\01fbb20e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\01fe3862-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\01fe3862-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\01fecc96-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\01fecc96-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\020cba40-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\020cba40-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\021fdb2e-1843-3f50-8d1a-489cac82f3e1.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\021fdb2e-1843-3f50-8d1a-489cac82f3e1.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\022b4b2c-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\022b4b2c-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0237f89a-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0237f89a-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\024e2eee-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\024e2eee-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02500ebc-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02500ebc-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\028bf968-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\028bf968-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02a94888-0c52-11ec-a887-8f4771ab3e45.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talake2\\AppData\\Local\\Temp\\ipykernel_34420\\2781878318.py:19: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imread(image_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02a94888-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02ae9770-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02ae9770-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02bcdd80-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02bcdd80-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02cb7fc0-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02cb7fc0-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02f0ae4e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\02f0ae4e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0300f47a-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0300f47a-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\03062698-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\03062698-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\031f9600-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\031f9600-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0332b8a2-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0332b8a2-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\03515e24-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\03515e24-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0356caa8-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0356caa8-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0392d548-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0392d548-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0393d1aa-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0393d1aa-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\03f2b81e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\03f2b81e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\041065da-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\041065da-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\041d0b14-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\041d0b14-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\043c3bb0-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\043c3bb0-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\045b24f8-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\045b24f8-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0492aa5e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0492aa5e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04a0190a-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04a0190a-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04d0320c-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04d0320c-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04d669d8-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04d669d8-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04e1ac90-c150-395b-a574-6a10b131e740.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04e1ac90-c150-395b-a574-6a10b131e740.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04e6dc7c-3dec-31ff-97b1-03cb3f6eb940.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04e6dc7c-3dec-31ff-97b1-03cb3f6eb940.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04ff6982-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\04ff6982-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0517dd64-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0517dd64-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\052ac082-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\052ac082-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\05372fe8-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\05372fe8-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0560bdb8-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0560bdb8-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\05953fde-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\05953fde-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\05d220a2-b7e7-4ca7-9793-bc292740f713.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\05d220a2-b7e7-4ca7-9793-bc292740f713.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\05dcce4e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\05dcce4e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06117702-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06117702-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\061436ea-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\061436ea-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\061a3608-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\061a3608-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\062222b4-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\062222b4-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\063a89bc-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\063a89bc-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06606f6a-1fec-31b4-9dd5-9af911519b60.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06606f6a-1fec-31b4-9dd5-9af911519b60.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\067727fa-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\067727fa-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\067cfc8e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\067cfc8e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0684b230-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0684b230-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0690122e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0690122e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06ab8784-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06ab8784-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06b5b6c8-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06b5b6c8-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06c4fd5e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06c4fd5e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06c5935e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06c5935e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06daf226-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\06daf226-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0718a7a6-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0718a7a6-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\073eeaf6-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\073eeaf6-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\074035a0-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\074035a0-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\074442b2-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\074442b2-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\074a4860-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\074a4860-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\074d7198-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\074d7198-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0758a3d8-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0758a3d8-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\075d622e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\075d622e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07642a11-63c5-3642-ac3a-7dd1c0df4bc7.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07642a11-63c5-3642-ac3a-7dd1c0df4bc7.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\077fef56-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\077fef56-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0780f4f8-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0780f4f8-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07811e76-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07811e76-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0781a448-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0781a448-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\078215ae-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\078215ae-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0782542e-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0782542e-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0783561c-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0783561c-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0784c830-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0784c830-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\079601f6-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\079601f6-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07978fa6-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07978fa6-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0797969a-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0797969a-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a14fca-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a14fca-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a1c19c-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a1c19c-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a20ea4-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a20ea4-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a22fba-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a22fba-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a51bb2-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a51bb2-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a597fe-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07a597fe-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07ab1684-1f14-11ec-81f2-eb8801c6f8d0.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07ab1684-1f14-11ec-81f2-eb8801c6f8d0.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07c131aa-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07c131aa-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07c9a394-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07c9a394-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07db56c0-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07db56c0-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07e4de52-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07e4de52-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07e7a81c-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07e7a81c-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07f553a4-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07f553a4-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07fc2b66-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\07fc2b66-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08116bb5-3d97-3dab-82f7-6d0a31795094.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08116bb5-3d97-3dab-82f7-6d0a31795094.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\081d3d4c-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\081d3d4c-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08250748-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08250748-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08313e5a-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08313e5a-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08315dea-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08315dea-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0842d85e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0842d85e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0845503e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0845503e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0848778c-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0848778c-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08603b24-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08603b24-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08b61dc8-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08b61dc8-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08b8ce74-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08b8ce74-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08d32f94-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08d32f94-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08fa9c50-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\08fa9c50-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0904df58-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0904df58-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0929df1a-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0929df1a-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\09301fba-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\09301fba-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0933b6fc-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0933b6fc-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\093be58e-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\093be58e-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\09471634-0c52-11ec-a887-8f4771ab3e45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\09471634-0c52-11ec-a887-8f4771ab3e45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\09e07fb2-9dd0-3e35-bc58-fa8db4e9edad.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\09e07fb2-9dd0-3e35-bc58-fa8db4e9edad.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0af08567-6cbd-3ad8-90ad-18c8df227c45.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0af08567-6cbd-3ad8-90ad-18c8df227c45.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0d2847fc-a692-300a-9871-0680f2cf289e.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0d2847fc-a692-300a-9871-0680f2cf289e.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0d384776-5de9-3a93-b271-3059ed875c7d.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0d384776-5de9-3a93-b271-3059ed875c7d.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0ee81be2-31c9-3c57-a423-db8f0ecdd05d.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0ee81be2-31c9-3c57-a423-db8f0ecdd05d.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0fb6de0a-c15a-369b-be5c-7e4ce621f998.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\0fb6de0a-c15a-369b-be5c-7e4ce621f998.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\100164657.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\100164657.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\100266158.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\100266158.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\100272623.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\100272623.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\100947385.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\100947385.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101124780.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101124780.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101231396.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101231396.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101265977.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101265977.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101383761.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101383761.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10159892.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10159892.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101633558.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101633558.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101880133.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\101880133.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\102578187.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\102578187.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\102908847.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\102908847.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\103297716.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\103297716.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\103369133.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\103369133.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\103758775.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\103758775.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\104014730.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\104014730.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10497885.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10497885.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\105035672.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\105035672.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\105041419.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\105041419.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10558812.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10558812.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\105839728.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\105839728.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1065062d-bfcf-3845-bc60-64215f6e81e6.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1065062d-bfcf-3845-bc60-64215f6e81e6.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10717487.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10717487.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\107529848.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\107529848.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10786764.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10786764.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\107869336.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\107869336.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10809974.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10809974.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10854312.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10854312.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\108733314.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\108733314.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10888878.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\10888878.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\109983584.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\109983584.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\110119590.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\110119590.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\110138169.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\110138169.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11143378.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11143378.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11208314.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11208314.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11279045.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11279045.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11286977.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11286977.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11350515.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11350515.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11409577.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11409577.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1146025.JPG\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1146025.JPG into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11651445.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11651445.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11660708.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11660708.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\117139837.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\117139837.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1171780.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1171780.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\117286841.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\117286841.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\118666120.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\118666120.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1188d3fc-f0c2-3e12-a706-fb9ea73c4aa5.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1188d3fc-f0c2-3e12-a706-fb9ea73c4aa5.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11c082a1-f99d-36f8-8d7c-1842d6777228.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\11c082a1-f99d-36f8-8d7c-1842d6777228.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\120284211.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\120284211.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\120456972.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\120456972.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\12048901.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\12048901.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\120660854.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\120660854.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\120997496.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\120997496.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\121651457.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\121651457.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\121870155.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\121870155.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\122120078.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\122120078.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\122276552.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\122276552.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\122841215.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\122841215.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\123135959.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\123135959.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\124106069.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\124106069.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\124380255.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\124380255.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\124919205.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\124919205.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\124995617.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\124995617.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125290751.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125290751.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125313249.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125313249.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125498623.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125498623.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125505817.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125505817.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125536222.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125536222.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125547565.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\125547565.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\126109497.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\126109497.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\12648697.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\12648697.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\12648736.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\12648736.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\128015909.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\128015909.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\128268708.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\128268708.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\12869909.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\12869909.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\129080803.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\129080803.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\129170550.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\129170550.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\129310371.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\129310371.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\129607840.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\129607840.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\129957242.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\129957242.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\130506821.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\130506821.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\130506884.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\130506884.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\130871843.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\130871843.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\131030838.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\131030838.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\131388150.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\131388150.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\131604578.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\131604578.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\132274991.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\132274991.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\132462431.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\132462431.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\132952982.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\132952982.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\133168728.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\133168728.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\133177787.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\133177787.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\134229399.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\134229399.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1356804.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1356804.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\136042244.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\136042244.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\13619383.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\13619383.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\13645876.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\13645876.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\13666418.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\13666418.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\136830341.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\136830341.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\137023096.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\137023096.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\137504665.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\137504665.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\137550016.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\137550016.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\137981669.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\137981669.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\138252448.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\138252448.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\138321476.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\138321476.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\138657821.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\138657821.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\138683114.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\138683114.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\138879213.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\138879213.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\139054760.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\139054760.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\139069146.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\139069146.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\139128469.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\139128469.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\139735124.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\139735124.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\141037086.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\141037086.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\141436660.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\141436660.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\141457292.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\141457292.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\141658990.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\141658990.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\141790916.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\141790916.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\142000713.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\142000713.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\143051668.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\143051668.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\143180634.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\143180634.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\143316816.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\143316816.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\143399463.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\143399463.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\143528491.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\143528491.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\144344338.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\144344338.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\144385903.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\144385903.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\144412484.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\144412484.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\145639441.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\145639441.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\146487777.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\146487777.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\147081406.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\147081406.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\147940124.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\147940124.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\148314254.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\148314254.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\148419212.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\148419212.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\148425687.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\148425687.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\148597742.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\148597742.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\149050770.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\149050770.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\149478328.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\149478328.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\14972130.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\14972130.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\150123077.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\150123077.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\150297495.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\150297495.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\150515535.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\150515535.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\150945701.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\150945701.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\151083717.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\151083717.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\151989824.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\151989824.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\152057892.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\152057892.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\152067743.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\152067743.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\152911021.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\152911021.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\153256617.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\153256617.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\15368456-4e7d-3e53-95c6-7ccb503b5459.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\15368456-4e7d-3e53-95c6-7ccb503b5459.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\153746752.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\153746752.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154051122.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154051122.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154257710.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154257710.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154441321.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154441321.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154575244.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154575244.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154863221.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154863221.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154969164.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\154969164.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\155374833.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\155374833.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\155381645.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\155381645.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\156167525.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\156167525.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\156804757.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\156804757.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\157394964.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\157394964.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\157649761.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\157649761.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\157948187.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\157948187.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\158760876.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\158760876.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\159115444.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\159115444.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\159651392.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\159651392.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\159672166.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\159672166.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\159918697.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\159918697.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\15e59e30-9e1a-3b33-bc15-0c621948f208.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\15e59e30-9e1a-3b33-bc15-0c621948f208.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160162123.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160162123.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160202105.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160202105.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160202115.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160202115.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160384371.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160384371.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160765403.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160765403.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160867451.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\160867451.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\161706274.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\161706274.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\161721020.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\161721020.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\162233140.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\162233140.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\162557897.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\162557897.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163255341.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163255341.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163296682.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163296682.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163305811.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163305811.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163494572.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163494572.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163554254.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163554254.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163585462.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163585462.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163588077.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\163588077.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164402196.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164402196.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164501488.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164501488.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164514323.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164514323.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164572406.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164572406.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164835146.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164835146.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164970812.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\164970812.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\165005278.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\165005278.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1653a994-8c3c-3f96-858b-51cb746fbb83.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1653a994-8c3c-3f96-858b-51cb746fbb83.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\165577137.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\165577137.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\165858145.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\165858145.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\166276686.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\166276686.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\166614228.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\166614228.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\166617213.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\166617213.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\166668152.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\166668152.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\166979424.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\166979424.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\167293301.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\167293301.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\167295124.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\167295124.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\167353940.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\167353940.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\168009250.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\168009250.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\168035459.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\168035459.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\168298386.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\168298386.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\168515597.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\168515597.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\169237787.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\169237787.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\16945388.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\16945388.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\16a863b5-9757-31a2-8b2f-ede6f2ef10c1.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\16a863b5-9757-31a2-8b2f-ede6f2ef10c1.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\171005223.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\171005223.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\171230580.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\171230580.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\174003369.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\174003369.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\174604746.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\174604746.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\175647197.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\175647197.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\17597745.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\17597745.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\17655101.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\17655101.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\17978963.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\17978963.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\179899198.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\179899198.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\18024774.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\18024774.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\18047263.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\18047263.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\181025764.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\181025764.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\181690716.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\181690716.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\182939535.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\182939535.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\18326092.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\18326092.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\184996874.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\184996874.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\185679951.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\185679951.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\18682550.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\18682550.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\186960329.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\186960329.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\187181732.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\187181732.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\187329870.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\187329870.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\18743509.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\18743509.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\188557495.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\188557495.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\190488118.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\190488118.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\19139385.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\19139385.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\191679728.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\191679728.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\191901708.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\191901708.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\191957910.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\191957910.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\192616999.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\192616999.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\192634349.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\192634349.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1929754.JPG\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1929754.JPG into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\19322679.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\19322679.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\19386043.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\19386043.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\194408307.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\194408307.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\194794798.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\194794798.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\19485499.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\19485499.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\194870976.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\194870976.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1954239.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1954239.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\195513395.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\195513395.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\196046997.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\196046997.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\19609308.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\19609308.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\196141012.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\196141012.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\196423524.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\196423524.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\196793310.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\196793310.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\197251816.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\197251816.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\198719420.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\198719420.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\198958770.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\198958770.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\199018803.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\199018803.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\199031999.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\199031999.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\199717096.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\199717096.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1a3def6f-a7d3-3e83-be6c-cdffc0782920.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1a3def6f-a7d3-3e83-be6c-cdffc0782920.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1ab89ccb-4993-31c0-8bcf-4db6fa429a42.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1ab89ccb-4993-31c0-8bcf-4db6fa429a42.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1c226287-deb7-3a5e-9ee5-960653a079d6.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1c226287-deb7-3a5e-9ee5-960653a079d6.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1d1dce11-83b2-30d4-8a6a-15256e8f2afb.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1d1dce11-83b2-30d4-8a6a-15256e8f2afb.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1d2e7ff3-2eb1-3318-81ba-73494b9f4d6a.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1d2e7ff3-2eb1-3318-81ba-73494b9f4d6a.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1d9c5a6f-f020-367b-9a43-d4c6b29434d7.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1d9c5a6f-f020-367b-9a43-d4c6b29434d7.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1ec4d8b3-7674-34e1-8674-441db3658672.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1ec4d8b3-7674-34e1-8674-441db3658672.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1ee54aba-f9a6-3f00-8471-c15f295dcc2c.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1ee54aba-f9a6-3f00-8471-c15f295dcc2c.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1f304d27-ca77-3612-9122-4877ca3ba62a.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1f304d27-ca77-3612-9122-4877ca3ba62a.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1f4861ee-44cc-3395-8abd-a4b0a56d5f08.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\1f4861ee-44cc-3395-8abd-a4b0a56d5f08.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\200579906.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\200579906.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\201100443.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\201100443.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\20208246.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\20208246.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\202227569.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\202227569.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\203173924.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\203173924.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\203639426.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\203639426.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\203973160.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\203973160.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\204953351.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\204953351.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\205114885.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\205114885.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\205789229.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\205789229.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\206329274.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\206329274.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\206399174.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\206399174.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\207111333.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\207111333.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\207200614.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\207200614.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\207200707.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\207200707.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\207257328.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\207257328.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\20759414.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\20759414.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\20811300.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\20811300.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\208361818.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\208361818.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\20943639.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\20943639.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\209882780.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\209882780.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\209977290.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\209977290.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\210052911.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\210052911.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\210228918.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\210228918.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\210369064.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\210369064.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21067902.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21067902.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\210926083.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\210926083.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21095383.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21095383.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\211420595.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\211420595.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\211420613.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\211420613.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\211494816.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\211494816.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\212134945.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\212134945.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\212336803.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\212336803.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2127378.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2127378.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\213261333.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\213261333.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\213978243.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\213978243.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2140927.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2140927.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\214126828.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\214126828.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\214733068.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\214733068.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21475411.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21475411.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21532610.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21532610.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\215549671.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\215549671.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\215935078.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\215935078.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21612556.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21612556.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216320325.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216320325.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216328826.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216328826.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216629039.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216629039.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216644164.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216644164.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216680264.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216680264.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216877631.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\216877631.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\218150038.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\218150038.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2190800.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2190800.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\219266719.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\219266719.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\219300100.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\219300100.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\219312311.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\219312311.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21982940.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\21982940.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\220167565.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\220167565.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\220337570.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\220337570.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\220484977.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\220484977.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\221019350.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\221019350.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\221f75c1-0a3e-3ba3-910b-ec54fa09ee65.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\221f75c1-0a3e-3ba3-910b-ec54fa09ee65.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\222031940.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\222031940.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\222293501.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\222293501.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\222494991.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\222494991.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\224560128.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\224560128.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\224854060.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\224854060.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\225965897.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\225965897.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\226284888.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\226284888.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\226658397.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\226658397.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\226692713.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\226692713.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\226840221.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\226840221.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\227081536.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\227081536.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\227198867.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\227198867.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\227516687.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\227516687.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\228434077.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\228434077.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\22856712.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\22856712.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\228583150.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\228583150.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\228924424.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\228924424.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\229012998.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\229012998.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\229229934.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\229229934.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\229940951.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\229940951.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230221208.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230221208.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230293580.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230293580.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230297653.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230297653.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230338841.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230338841.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230870927.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230870927.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230904349.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\230904349.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\231062112.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\231062112.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\23107106.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\23107106.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\231325441.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\231325441.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\23170372.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\23170372.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\231783769.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\231783769.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\232152139.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\232152139.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\232324481.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\232324481.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\232653837.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\232653837.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\232902520.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\232902520.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2329030.JPG\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2329030.JPG into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\232977631.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\232977631.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233205184.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233205184.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233563607.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233563607.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233564612.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233564612.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2335987.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2335987.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233598824.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233598824.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233761445.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233761445.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233946034.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\233946034.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\23406720.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\23406720.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234186809.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234186809.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234506736.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234506736.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234516137.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234516137.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234516168.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234516168.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234535222.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234535222.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234608968.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234608968.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234617659.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234617659.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234620958.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234620958.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234792457.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234792457.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234820059.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\234820059.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\235168662.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\235168662.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\235559845.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\235559845.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\23646977.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\23646977.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\236534257.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\236534257.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\236614091.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\236614091.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2371997.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2371997.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\237336612.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\237336612.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\237620346.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\237620346.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\238350977.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\238350977.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\238381090.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\238381090.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\239184257.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\239184257.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\239210705.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\239210705.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\23933937.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\23933937.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\239372310.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\239372310.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\239980096.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\239980096.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\239982478.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\239982478.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\240429843.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\240429843.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\240619180.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\240619180.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\240645019.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\240645019.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\241065043.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\241065043.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24209128-6097-38e1-999b-ad3c477fd8be.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24209128-6097-38e1-999b-ad3c477fd8be.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\242098014.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\242098014.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\242676539.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\242676539.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\242679543.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\242679543.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\243535193.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\243535193.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\244006384.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\244006384.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2443051.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2443051.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\244307907.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\244307907.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\244424259.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\244424259.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24445635.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24445635.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24451804.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24451804.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2447509.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2447509.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\245144162.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\245144162.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\245199628.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\245199628.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24540982.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24540982.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24541036.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24541036.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\245590481.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\245590481.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2458533.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2458533.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\246146244.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\246146244.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24746353.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\24746353.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\247506317.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\247506317.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2480213.JPG\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2480213.JPG into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2484797.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2484797.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2484798.jpeg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2484798.jpeg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2484907.jpg\n",
      "Incorporated D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\2484907.jpg into statistics calculation.\n",
      "Processing image: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_data_top1000_may724_curated\\acer\\248722999.jpg\n"
     ]
    }
   ],
   "source": [
    "# Path to iNat images\n",
    "inat_path = training_destination_root\n",
    "\n",
    "# Transforms to preprocess iNaturalist images for CNN\n",
    "transform = v2.Compose([\n",
    "    v2.ToPILImage(),\n",
    "    v2.Resize(size=(512, 512), antialias=True),\n",
    "    v2.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize variables to accumulate mean and standard deviation values\n",
    "mean_sum = torch.zeros(3)\n",
    "std_sum = torch.zeros(3)\n",
    "num_images = 0\n",
    "\n",
    "# Function to process each image\n",
    "def process_image(image_path):\n",
    "    try:\n",
    "        img = imread(image_path)\n",
    "        img = transform(img)  # Transforms image to [0-1] and channels first\n",
    "        if torch.isnan(img).any():\n",
    "            print(f\"NaN values found after transformation in {image_path}\")\n",
    "            return\n",
    "\n",
    "        img = img.permute(1, 2, 0)  # Transforms image to [512, 512, 3] dimensions\n",
    "        # Compute mean and standard deviation for each channel\n",
    "        mean = torch.mean(img, dim=(0, 1))\n",
    "        std = torch.std(img, dim=(0, 1))\n",
    "\n",
    "        if torch.isnan(mean).any() or torch.isnan(std).any():\n",
    "            print(f\"NaN values found in mean or std calculation for {image_path}\")\n",
    "            return\n",
    "\n",
    "        return mean, std\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return\n",
    "\n",
    "# Walk through the directory structure\n",
    "for root, dirs, files in os.walk(inat_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(root, file)\n",
    "            print(f\"Processing image: {image_path}\")  # Log each image being processed\n",
    "            result = process_image(image_path)\n",
    "            if result:\n",
    "                mean, std = result\n",
    "                # Accumulate mean and standard deviation values\n",
    "                mean_sum += mean\n",
    "                std_sum += std\n",
    "                num_images += 1\n",
    "                print(f\"Incorporated {image_path} into statistics calculation.\")  # Log successful incorporation\n",
    "\n",
    "# Calculate average mean and standard deviation across all images\n",
    "if num_images > 0:\n",
    "    avg_mean = mean_sum / num_images\n",
    "    avg_std = std_sum / num_images\n",
    "    print(\"Average Mean:\", avg_mean)\n",
    "    print(\"Average Std:\", avg_std)\n",
    "else:\n",
    "    print(\"No valid images processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82427373",
   "metadata": {},
   "source": [
    "### Base Trained EfficientNetV2 Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d10ace",
   "metadata": {},
   "source": [
    "#### Define Image Augumentations - Update paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfd4eee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 44899\n",
       "    Root location: D:\\blaginh\\tree_classification\\aa_inat_combined\\training_dataset_small_march624\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "                 ToImage()\n",
       "                 RandomResizedCrop(size=(512, 512), scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
       "                 RandomHorizontalFlip(p=0.5)\n",
       "                 ToDtype(scale=True)\n",
       "                 Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       "           )"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512]) 0\n"
     ]
    }
   ],
   "source": [
    "# Use Pytorch ImageFolder class to prepare training and testing datasets\n",
    "train_data_dir = training_destination_root\n",
    "test_data_dir = testing_destination_root\n",
    "test_data_dir_aa = testing_destination_root_aa\n",
    "test_data_dir_inat = testing_destination_root_inat\n",
    "\n",
    "# Load the training and testing datasets as Pytorch Dataset Classes: https://pytorch.org/docs/stable/data.html\n",
    "# The Pytorch torchvision.transforms module provides preprocessing functions: https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomResizedCrop(size=(512, 512), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Ensure images are resized during testing as same dimension for training\n",
    "test_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize(size=(512, 512), antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(train_data_dir, transform = train_transforms)\n",
    "test_dataset = ImageFolder(test_data_dir, transform = test_transforms)\n",
    "test_dataset_aa = ImageFolder(test_data_dir_aa, transform = test_transforms)\n",
    "test_dataset_inat = ImageFolder(test_data_dir_inat, transform = test_transforms)\n",
    "\n",
    "# Examine the train_dataset object\n",
    "train_dataset\n",
    "\n",
    "# Examine image dimensions: (3 channels, height 64, width 64)\n",
    "img, label = train_dataset[0]\n",
    "print(img.shape,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ddbf90",
   "metadata": {},
   "source": [
    "#### Define training, validation, and testing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into a validation set, and prepare dataset for training\n",
    "\n",
    "# Define batch size for training \n",
    "bs = 32\n",
    "\n",
    "# Define number of images for validation (typically, 10% of the training set)\n",
    "val_size = 2000\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "# Randomly split training data into train_data and val_data sets\n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Length of Train Data : {len(train_data)}\") # Length of Train Data : 20000\n",
    "print(f\"Length of Validation Data : {len(val_data)}\") # Length of Validation Data : 2000\n",
    "\n",
    "# Use Pytorch DataLoader Class to iterate over a dataset for training: https://pytorch.org/docs/stable/data\n",
    "\n",
    "train_dl = DataLoader(dataset = train_data, batch_size = bs, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(dataset = val_data, batch_size = bs*2, num_workers = 4, pin_memory = True)\n",
    "test_dl = DataLoader(dataset = test_dataset, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "test_dl_aa = DataLoader(dataset = test_dataset_aa, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "test_dl_inat = DataLoader(dataset = test_dataset_inat, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "\n",
    "# Instantiate the model\n",
    "model = EfficientNetImageClassification()\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f'Number of Model Parameters: ', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f84b94",
   "metadata": {},
   "source": [
    "#### Visualize model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab0e67",
   "metadata": {},
   "source": [
    "##### Load Pre-Trained Model and Setup for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "model_path = r\"D:\\blaginh\\tree_classification\\model_outputs\\base\\tree-classification-autoarb_inat-25-genera-1000imgs-effnet2s-10epochs-lr001-aug-march724.pth\"\n",
    "\n",
    "# Get GPU Device\n",
    "device = get_default_device()\n",
    "device\n",
    "\n",
    "# Instantiate the model with the same architecture as the model which parameters you saved\n",
    "model = EfficientNetImageClassification()\n",
    "\n",
    "#load the model to the device\n",
    "model = to_device(EfficientNetImageClassification(), device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval() # Call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51567262",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (Autoarborist & iNaturalist) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa958e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\base\\base_confusion_matrix_all.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1a3d3",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (Autoarborist & iNaturalist) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698984a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera))\n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\base\\base_classification_report_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15040376",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (Autoarborist Only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0698a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl_aa:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\base\\base_confusion_matrix_aaonly.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072dbe4",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (Autoarborist only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eecddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera))\n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\base\\base_classification_report_aaonly.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa37c6f",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (iNaturalist Only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be6815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl_inat:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\base\\base_confusion_matrix_inatonly.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c55cc",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (iNaturalist only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e12db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera))\n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\base\\base_classification_report_inatonly.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6891a53",
   "metadata": {},
   "source": [
    "### Experiment 1: The effect of decreased resolution (512x512) on CNN image classification performance.\n",
    "\n",
    "Hypothesis: decreased resolution will result in decreased performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b92ec",
   "metadata": {},
   "source": [
    "#### Define Image Augumentations - Update paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pytorch ImageFolder class to prepare training and testing datasets\n",
    "train_data_dir = training_destination_root\n",
    "test_data_dir = testing_destination_root\n",
    "test_data_dir_aa = testing_destination_root_aa\n",
    "test_data_dir_inat = testing_destination_root_inat\n",
    "\n",
    "# Load the training and testing datasets as Pytorch Dataset Classes: https://pytorch.org/docs/stable/data.html\n",
    "# The Pytorch torchvision.transforms module provides preprocessing functions: https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    #v2.RandomResizedCrop(size=(512, 512), antialias=True),\n",
    "    v2.RandomResizedCrop(size=(256, 256), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Ensure images are resized during testing as same dimension for training\n",
    "test_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    #v2.Resize(size=(512, 512), antialias=True),\n",
    "    v2.Resize(size=(256, 256), antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(train_data_dir, transform = train_transforms)\n",
    "test_dataset = ImageFolder(test_data_dir, transform = test_transforms)\n",
    "test_dataset_aa = ImageFolder(test_data_dir_aa, transform = test_transforms)\n",
    "test_dataset_inat = ImageFolder(test_data_dir_inat, transform = test_transforms)\n",
    "\n",
    "# Examine the train_dataset object\n",
    "train_dataset\n",
    "\n",
    "# Examine image dimensions: (3 channels, height 64, width 64)\n",
    "img, label = train_dataset[0]\n",
    "print(img.shape,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b78453",
   "metadata": {},
   "source": [
    "#### Define training, validation, and testing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe26fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into a validation set, and prepare dataset for training\n",
    "\n",
    "# Define batch size for training \n",
    "bs = 32\n",
    "\n",
    "# Define number of images for validation (typically, 10% of the training set)\n",
    "val_size = 2000\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "# Randomly split training data into train_data and val_data sets\n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Length of Train Data : {len(train_data)}\") # Length of Train Data : 20000\n",
    "print(f\"Length of Validation Data : {len(val_data)}\") # Length of Validation Data : 2000\n",
    "\n",
    "# Use Pytorch DataLoader Class to iterate over a dataset for training: https://pytorch.org/docs/stable/data\n",
    "\n",
    "train_dl = DataLoader(dataset = train_data, batch_size = bs, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(dataset = val_data, batch_size = bs*2, num_workers = 4, pin_memory = True)\n",
    "test_dl = DataLoader(dataset = test_dataset, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "test_dl_aa = DataLoader(dataset = test_dataset_aa, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "test_dl_inat = DataLoader(dataset = test_dataset_inat, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "\n",
    "# Instantiate the model\n",
    "model = EfficientNetImageClassification()\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f'Number of Model Parameters: ', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1411ca9",
   "metadata": {},
   "source": [
    "#### Print number of classes in train and test datasets, and visualize sample image and single batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many classes are in the training and testing datasets?\n",
    "print(\"Classes in the Training Dataset : /n\", len(train_dataset.classes))\n",
    "print(\"Classes in the Testing Dataset : /n\", len(test_dataset.classes))\n",
    "print(\"Classes in the Testing Dataset (Autoarborist Only) : /n\", len(test_dataset_aa.classes))\n",
    "\n",
    "# Display the first image in the dataset\n",
    "display_img(*train_dataset[2])\n",
    "\n",
    "# Visualize a single batch of images\n",
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ae4680",
   "metadata": {},
   "source": [
    "#### Load data to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9457c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU Device\n",
    "device = get_default_device()\n",
    "device\n",
    "\n",
    "# Load data to GPU\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "to_device(model, device)\n",
    "\n",
    "# Load the model to the GPU\n",
    "model = to_device(EfficientNetImageClassification(), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028331b",
   "metadata": {},
   "source": [
    "#### Fit and Save Model & Model History - update paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee900cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and record result after epoch\n",
    "# Model history contains training loss, validation loss, and validation accuracy metrics\n",
    "history = fit(num_epochs, base_lr, model, train_dl, val_dl, opt_func)\n",
    "\n",
    "# Save the model weights file to path\n",
    "model_path = r'D:\\blaginh\\tree_classification\\model_outputs\\exp1_reduced_res\\tree-classification-autoarb_inat-25-genera-1000imgs-effnet2s-10epochs-lr001-aug-apr1124.pth'\n",
    "\n",
    "# Torch.Save model to file: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "#export the model history to a csv file\n",
    "history_path = r'D:\\blaginh\\tree_classification\\model_outputs\\exp1_reduced_res\\tree-classification-autoarb_inat-25-genera-1000imgs-effnet2s-10epochs-lr001-aug-apr1124.csv'\n",
    "history_df = pd.DataFrame(history)\n",
    "#add column for epoch\n",
    "history_df['epoch'] = history_df.index + 1\n",
    "history_df.to_csv(history_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d70248",
   "metadata": {},
   "source": [
    "#### Visualize model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6789f",
   "metadata": {},
   "source": [
    "##### Accuracy and loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637273df",
   "metadata": {},
   "source": [
    "##### Load Pre-Trained Model and Setup for Model Evaluation - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "model_path = r'D:\\blaginh\\tree_classification\\model_outputs\\exp1_reduced_res\\\\tree-classification-autoarb_inat-25-genera-1000imgs-effnet2s-10epochs-lr001-aug-apr1124.pth'\n",
    "\n",
    "# Get GPU Device\n",
    "device = get_default_device()\n",
    "device\n",
    "\n",
    "# Instantiate the model with the same architecture as the model which parameters you saved\n",
    "model = EfficientNetImageClassification()\n",
    "\n",
    "#load the model to the device\n",
    "model = to_device(EfficientNetImageClassification(), device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval() # Call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24cf5d",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (Autoarborist & iNaturalist) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795bea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp1_reduced_res\\exp1_confusion_matrix_all.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb4fb0",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (Autoarborist & iNaturalist) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera))\n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp1_reduced_res\\exp1_classification_report_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7acc3",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (Autoarborist Only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d94d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl_aa:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp1_reduced_res\\exp1_confusion_matrix_aaonly.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe09d12a",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (Autoarborist only) - updath path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52eb0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera))\n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp1_reduced_res\\exp1_classification_report_aaonly.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0b166",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (iNaturalist Only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl_inat:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp1_reduced_res\\exp1_confusion_matrix_inatonly.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72073ad",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (iNaturalist only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81834820",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera))\n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp1_reduced_res\\exp1_classification_report_inatonly.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2136907",
   "metadata": {},
   "source": [
    "### Experiment 2: The effect of increased resolution (768x768) on CNN image classification performance.\n",
    "\n",
    "Hypothesis: increased resolution will result in increased performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f90a14",
   "metadata": {},
   "source": [
    "#### Define Image Augumentations - Update paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d18bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pytorch ImageFolder class to prepare training and testing datasets\n",
    "train_data_dir = training_destination_root\n",
    "test_data_dir = testing_destination_root\n",
    "test_data_dir_aa = testing_destination_root_aa\n",
    "test_data_dir_inat = testing_destination_root_inat\n",
    "\n",
    "# Load the training and testing datasets as Pytorch Dataset Classes: https://pytorch.org/docs/stable/data.html\n",
    "# The Pytorch torchvision.transforms module provides preprocessing functions: https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    #v2.RandomResizedCrop(size=(512, 512), antialias=True),\n",
    "    v2.RandomResizedCrop(size=(768, 768), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Ensure images are resized during testing as same dimension for training\n",
    "test_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    #v2.Resize(size=(512, 512), antialias=True),\n",
    "    v2.Resize(size=(768, 768), antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(train_data_dir, transform = train_transforms)\n",
    "test_dataset = ImageFolder(test_data_dir, transform = test_transforms)\n",
    "test_dataset_aa = ImageFolder(test_data_dir_aa, transform = test_transforms)\n",
    "test_dataset_inat = ImageFolder(test_data_dir_inat, transform = test_transforms)\n",
    "\n",
    "# Examine the train_dataset object\n",
    "train_dataset\n",
    "\n",
    "# Examine image dimensions: (3 channels, height 64, width 64)\n",
    "img, label = train_dataset[0]\n",
    "print(img.shape,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405d862",
   "metadata": {},
   "source": [
    "#### Define training, validation, and testing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad667f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into a validation set, and prepare dataset for training\n",
    "\n",
    "# Define batch size for training \n",
    "bs = 32\n",
    "\n",
    "# Define number of images for validation (typically, 10% of the training set)\n",
    "val_size = 2000\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "# Randomly split training data into train_data and val_data sets\n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Length of Train Data : {len(train_data)}\") # Length of Train Data : 20000\n",
    "print(f\"Length of Validation Data : {len(val_data)}\") # Length of Validation Data : 2000\n",
    "\n",
    "# Use Pytorch DataLoader Class to iterate over a dataset for training: https://pytorch.org/docs/stable/data\n",
    "\n",
    "train_dl = DataLoader(dataset = train_data, batch_size = bs, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(dataset = val_data, batch_size = bs*2, num_workers = 4, pin_memory = True)\n",
    "test_dl = DataLoader(dataset = test_dataset, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "test_dl_aa = DataLoader(dataset = test_dataset_aa, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "test_dl_inat = DataLoader(dataset = test_dataset_inat, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "\n",
    "# Instantiate the model\n",
    "model = EfficientNetImageClassification()\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f'Number of Model Parameters: ', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f3359",
   "metadata": {},
   "source": [
    "#### Print number of classes in train and test datasets, and visualize sample image and single batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many classes are in the training and testing datasets?\n",
    "print(\"Classes in the Training Dataset : /n\", len(train_dataset.classes))\n",
    "print(\"Classes in the Testing Dataset : /n\", len(test_dataset.classes))\n",
    "print(\"Classes in the Testing Dataset (Autoarborist Only) : /n\", len(test_dataset_aa.classes))\n",
    "print(\"Classes in the Testing Dataset (iNaturalist Only) : /n\", len(test_dataset_inat.classes))\n",
    "\n",
    "# Display the first image in the dataset\n",
    "display_img(*train_dataset[2])\n",
    "\n",
    "# Visualize a single batch of images\n",
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc31a5",
   "metadata": {},
   "source": [
    "#### Load data to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e38bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU Device\n",
    "device = get_default_device()\n",
    "device\n",
    "\n",
    "# Load data to GPU\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "to_device(model, device)\n",
    "\n",
    "# Load the model to the GPU\n",
    "model = to_device(EfficientNetImageClassification(), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d19980",
   "metadata": {},
   "source": [
    "#### Fit and Save Model & Model History - update paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and record result after epoch\n",
    "# Model history contains training loss, validation loss, and validation accuracy metrics\n",
    "history = fit(num_epochs, base_lr, model, train_dl, val_dl, opt_func)\n",
    "\n",
    "# Save the model weights file to path\n",
    "model_path = r'D:\\blaginh\\tree_classification\\model_outputs\\exp2_increased_res\\tree-classification-autoarb_inat-25-genera-1000imgs-effnet2s-10epochs-lr001-aug-apr1124.pth'\n",
    "\n",
    "# Torch.Save model to file: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "#export the model history to a csv file\n",
    "history_path = r'D:\\blaginh\\tree_classification\\model_outputs\\exp2_increased_res\\tree-classification-autoarb_inat-25-genera-1000imgs-effnet2s-10epochs-lr001-aug-apr1124.csv'\n",
    "history_df = pd.DataFrame(history)\n",
    "#add column for epoch\n",
    "history_df['epoch'] = history_df.index + 1\n",
    "history_df.to_csv(history_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a67f0",
   "metadata": {},
   "source": [
    "#### Visualize model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1ce52",
   "metadata": {},
   "source": [
    "##### Accuracy and loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e56295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349747d7",
   "metadata": {},
   "source": [
    "##### Load Pre-Trained Model and Setup for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "model_path = r'D:\\blaginh\\tree_classification\\model_outputs\\exp2_increased_res\\\\tree-classification-autoarb_inat-25-genera-1000imgs-effnet2s-10epochs-lr001-aug-apr1124.pth'\n",
    "\n",
    "# Get GPU Device\n",
    "device = get_default_device()\n",
    "device\n",
    "\n",
    "# Instantiate the model with the same architecture as the model which parameters you saved\n",
    "model = EfficientNetImageClassification()\n",
    "\n",
    "#load the model to the device\n",
    "model = to_device(EfficientNetImageClassification(), device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval() # Call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8663e",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (Autoarborist & iNaturalist) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8123e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp2_increased_res\\exp2_confusion_matrix_all.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e96664",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (Autoarborist & iNaturalist) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e81f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera))\n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp2_increased_res\\exp2_classification_report_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b726823",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (Autoarborist Only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl_aa:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp2_increased_res\\exp2_confusion_matrix_aaonly.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468f9b7",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (Autoarborist only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e924cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera))\n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp2_increased_res\\exp2_classification_report_aaonly.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69998f83",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (iNaturalist Only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7209d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl_inat:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp2_increased_res\\exp2_confusion_matrix_inatonly.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef016e",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (iNaturalist only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera))\n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp2_increased_res\\exp2_classification_report_inatonly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the model history to a csv file\n",
    "history_path = r'D:\\blaginh\\tree_classification\\model_outputs\\exp2_increased_res\\tree-classification-autoarb_inat-25-genera-1000imgs-effnet2s-10epochs-lr001-aug-apr1124.csv'\n",
    "history_df = pd.DataFrame(history)\n",
    "#add column for epoch\n",
    "history_df['epoch'] = history_df.index + 1\n",
    "history_df.to_csv(history_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad497f",
   "metadata": {},
   "source": [
    "### Experiment 3: The effect of random crop on CNN image classification performance.\n",
    "\n",
    "Hypothesis: random cropping will increased the model's performance on classifying autoarborist, decrease the model's performance on classifying inaturalist, and decreasing the model's performance on classifying autoarborist + inaturalist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421342f7",
   "metadata": {},
   "source": [
    "#### Define Image Augumentations - Update paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b0245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pytorch ImageFolder class to prepare training and testing datasets\n",
    "train_data_dir = training_destination_root\n",
    "test_data_dir = testing_destination_root\n",
    "test_data_dir_aa = testing_destination_root_aa\n",
    "test_data_dir_inat = testing_destination_root_inat\n",
    "\n",
    "# Load the training and testing datasets as Pytorch Dataset Classes: https://pytorch.org/docs/stable/data.html\n",
    "# The Pytorch torchvision.transforms module provides preprocessing functions: https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomCrop(size=(512,512), pad_if_needed = True),\n",
    "    #v2.RandomResizedCrop(size=(512, 512), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Ensure images are resized during testing as same dimension for training\n",
    "test_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize(size=(512, 512), antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(train_data_dir, transform = train_transforms)\n",
    "test_dataset = ImageFolder(test_data_dir, transform = test_transforms)\n",
    "test_dataset_aa = ImageFolder(test_data_dir_aa, transform = test_transforms)\n",
    "test_dataset_inat = ImageFolder(test_data_dir_inat, transform = test_transforms)\n",
    "\n",
    "# Examine the train_dataset object\n",
    "train_dataset\n",
    "\n",
    "# Examine image dimensions: (3 channels, height 64, width 64)\n",
    "img, label = train_dataset[0]\n",
    "print(img.shape,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63d4b7",
   "metadata": {},
   "source": [
    "#### Define training, validation, and testing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a84e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into a validation set, and prepare dataset for training\n",
    "\n",
    "# Define batch size for training \n",
    "bs = 32\n",
    "\n",
    "# Define number of images for validation (typically, 10% of the training set)\n",
    "val_size = 2000\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "# Randomly split training data into train_data and val_data sets\n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Length of Train Data : {len(train_data)}\") # Length of Train Data : 20000\n",
    "print(f\"Length of Validation Data : {len(val_data)}\") # Length of Validation Data : 2000\n",
    "\n",
    "# Use Pytorch DataLoader Class to iterate over a dataset for training: https://pytorch.org/docs/stable/data\n",
    "\n",
    "train_dl = DataLoader(dataset = train_data, batch_size = bs, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(dataset = val_data, batch_size = bs*2, num_workers = 4, pin_memory = True)\n",
    "test_dl = DataLoader(dataset = test_dataset, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "test_dl_aa = DataLoader(dataset = test_dataset_aa, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "test_dl_inat = DataLoader(dataset = test_dataset_inat, batch_size = 1, num_workers = 4, pin_memory = True)\n",
    "\n",
    "# Instantiate the model\n",
    "model = EfficientNetImageClassification()\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f'Number of Model Parameters: ', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33fa45",
   "metadata": {},
   "source": [
    "#### Print number of classes in train and test datasets, and visualize sample image and single batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many classes are in the training and testing datasets?\n",
    "print(\"Classes in the Training Dataset : /n\", len(train_dataset.classes))\n",
    "print(\"Classes in the Testing Dataset : /n\", len(test_dataset.classes))\n",
    "print(\"Classes in the Testing Dataset (Autoarborist Only) : /n\", len(test_dataset_aa.classes))\n",
    "print(\"Classes in the Testing Dataset (iNaturalist Only) : /n\", len(test_dataset_inat.classes))\n",
    "\n",
    "# Display the first image in the dataset\n",
    "display_img(*train_dataset[2])\n",
    "\n",
    "# Visualize a single batch of images\n",
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930907b1",
   "metadata": {},
   "source": [
    "#### Load data to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc64ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU Device\n",
    "device = get_default_device()\n",
    "device\n",
    "\n",
    "# Load data to GPU\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "to_device(model, device)\n",
    "\n",
    "# Load the model to the GPU\n",
    "model = to_device(EfficientNetImageClassification(), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037bccb1",
   "metadata": {},
   "source": [
    "#### Fit and Save Model & Model History - update paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and record result after epoch\n",
    "# Model history contains training loss, validation loss, and validation accuracy metrics\n",
    "history = fit(num_epochs, base_lr, model, train_dl, val_dl, opt_func)\n",
    "\n",
    "# Save the model weights file to path\n",
    "model_path = r\"D:\\blaginh\\tree_classification\\model_outputs\\exp3_randomcrop\\tree-classification-autoarb_inat-25-genera-1000imgs-effnet2s-10epochs-lr001-aug-apr1124.pth\"\n",
    "\n",
    "# Torch.Save model to file: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "#export the model history to a csv file\n",
    "history_path = r\"D:\\blaginh\\tree_classification\\model_outputs\\exp3_randomcrop\\tree-classification-autoarb_inat-25-genera-1000imgs-effnet2s-10epochs-lr001-aug-apr1124.csv\"\n",
    "history_df = pd.DataFrame(history)\n",
    "#add column for epoch\n",
    "history_df['epoch'] = history_df.index + 1\n",
    "history_df.to_csv(history_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d18832",
   "metadata": {},
   "source": [
    "#### Visualize model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52272d52",
   "metadata": {},
   "source": [
    "##### Accuracy and loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8691c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a2c23",
   "metadata": {},
   "source": [
    "##### Load Pre-Trained Model and Setup for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "model_path = r\"D:\\blaginh\\tree_classification\\model_outputs\\exp3_randomcrop\\\\tree-classification-autoarb_inat-25-genera-1000imgs-effnet2s-10epochs-lr001-aug-apr1124.pth\"\n",
    "\n",
    "# Get GPU Device\n",
    "device = get_default_device()\n",
    "device\n",
    "\n",
    "# Instantiate the model with the same architecture as the model which parameters you saved\n",
    "model = EfficientNetImageClassification()\n",
    "\n",
    "#load the model to the device\n",
    "model = to_device(EfficientNetImageClassification(), device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval() # Call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a76059d",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (Autoarborist & iNaturalist) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a834b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp3_randomcrop\\exp3_confusion_matrix_all.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1d5df",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (Autoarborist & iNaturalist) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera))\n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp3_randomcrop\\exp3_classification_report_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff67465",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (Autoarborist Only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl_aa:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp3_randomcrop\\exp3_confusion_matrix_aaonly.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37486e8a",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (Autoarborist only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6282124",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera))\n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp3_randomcrop\\exp3_classification_report_aaonly.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f021065",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: All Images (iNaturalist Only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534bdc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation with Confusion Matrix\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl_inat:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output = model(inputs)\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()  # Extract predicted labels directly\n",
    "        y_pred.extend(output)\n",
    "        \n",
    "        labels = labels.cpu().numpy()\n",
    "        y_true.extend(labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(cf_matrix, display_labels=selected_genera)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "plt.savefig(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp3_randomcrop\\exp3_confusion_matrix_inatonly.png\", dpi=200)\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90987ce2",
   "metadata": {},
   "source": [
    "##### Precision, recall, F1, support per class: All Images (iNaturalist only) - update path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names = selected_genera)) \n",
    "\n",
    "# Export classification report to csv\n",
    "report_data = classification_report(y_true, y_pred, target_names = selected_genera, output_dict=True)\n",
    "report_df = pd.DataFrame(report_data).transpose()\n",
    "report_df.to_csv(r\"D:\\blaginh\\tree_classification\\model_outputs\\exp3_randomcrop\\exp3_classification_report_inatonly.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
